{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% library import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch as tc\n",
    "import torch\n",
    "import dgl\n",
    "import pickle\n",
    "import time\n",
    "import mdtraj\n",
    "\n",
    "from rdkit.Chem import AllChem as chem\n",
    "from rdkit.Chem import Descriptors as descriptor\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "from dgl import DGLGraph\n",
    "from dgl import function as fn\n",
    "from dgl.data.chem import utils\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "#%% Load dataset and cuda\n",
    "dataset = pd.read_csv(\"datasets/KIBA.csv\")\n",
    "datalen = len(dataset)\n",
    "cuda = tc.device('cuda')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% protein-ligand-kiba split\n",
    "protein = dataset.loc[:(2**16)+(2**13)-1, \"uniprotID\"]    # 5\n",
    "ligand = dataset.loc[:(2**16)+(2**13)-1, \"chemblID\"]\n",
    "kiba = list(dataset.loc[:(2**16)+(2**13)-1, 'KIBA'])\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% protein sequence load\n",
    "f = open('datasets/dictionaries/prt_lstm.txt', 'rb')\n",
    "seq_voc, _ = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "sequence = np.zeros(((2**16)+(2**13), 4128))\n",
    "for i, s in enumerate(protein):\n",
    "    sequence[i] = seq_voc[s]\n",
    "\n",
    "sequence = sequence[:, :2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ligand ecfp and graph load\n",
    "f = open('datasets/dictionaries/lgn_ecfp.txt', 'rb')\n",
    "ecfp = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "ecfprint = np.zeros(((2**16)+(2**13), 2048))\n",
    "for i, c in enumerate(ligand):\n",
    "    ecfprint[i] = ecfp[c]\n",
    "\n",
    "f = open('datasets/dictionaries/lgn_smiles.txt', 'rb')\n",
    "smiles = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "graph = []\n",
    "for i, c in enumerate(ligand):\n",
    "    graph.append(utils.smiles_to_bigraph(smiles[c]).to(torch.device('cuda:0')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% dataset zip\n",
    "revised_dataset = list(zip(sequence, ecfprint, graph, kiba))\n",
    "shuffled_dataset = shuffle(revised_dataset); del revised_dataset\n",
    "trainset = shuffled_dataset[:2**16]\n",
    "validset = shuffled_dataset[2**16:(2**16) + (2**13)]\n",
    "\n",
    "del shuffled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Make collate func.\n",
    "def collate(samples):\n",
    "    sequences, ecfprints, graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs).to(torch.device('cuda:0'))\n",
    "    return tc.LongTensor(sequences).cuda(), tc.tensor(ecfprints, dtype=tc.float).cuda(), batched_graph, tc.tensor(labels).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% GCN module \n",
    "msg = fn.copy_src(src='h', out='m')\n",
    "\n",
    "def reduce(nodes):\n",
    "    \"\"\"Take an average over all neighbor node features hu and use it to\n",
    "    overwrite the original node feature.\"\"\"\n",
    "    accum = tc.mean(nodes.mailbox['m'], 1).cuda()\n",
    "    return {'h': accum}\n",
    "\n",
    "class NodeApplyModule(nn.Module):\n",
    "    \"\"\"Update the node feature hv with ReLU(Whv+b).\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        h = self.activation(h)\n",
    "        return {'h' : h}\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        # Initialize the node features with h.\n",
    "        g.ndata['h'] = feature.cuda()\n",
    "        g.update_all(msg, reduce)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% learning module 선언\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Regressor, self).__init__()    # method 상속받고 __init__()은 여기서 하겠다.\n",
    "        \n",
    "        self.emlayer = nn.Embedding(21, 10)\n",
    "        \n",
    "        self.cv2dlayer = nn.Sequential(\n",
    "                        nn.Conv2d(1, 8, kernel_size = (64, 10), stride=(2, 1)),\n",
    "                        nn.BatchNorm2d(num_features = 8),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size = (2, 1))\n",
    "                        )\n",
    "        \n",
    "        self.cv1dlayers = nn.Sequential(\n",
    "                        nn.Conv1d(8, 16, kernel_size = 3),\n",
    "                        nn.BatchNorm1d(num_features = 16),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool1d(kernel_size = 2),\n",
    "                        nn.Conv1d(16, 24, kernel_size = 2),\n",
    "                        nn.BatchNorm1d(num_features = 24),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool1d(kernel_size = 2)\n",
    "                        )\n",
    "            \n",
    "        self.lslayer = nn.LSTM(24, 64, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.eclayers = nn.Sequential(\n",
    "                        nn.Linear(2048, 1024),\n",
    "                        nn.BatchNorm1d(1024),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.3),\n",
    "                        nn.Linear(1024, 512),\n",
    "                        nn.BatchNorm1d(512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.3),\n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.BatchNorm1d(256),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.3),\n",
    "                        nn.Linear(256, 64),\n",
    "                        )\n",
    "        \n",
    "        self.gclayers = nn.ModuleList([\n",
    "                        GCN(1, 64, F.relu),\n",
    "                        GCN(64, 128, F.relu),\n",
    "                        GCN(128, 64, F.relu)]\n",
    "                        )\n",
    "\n",
    "        self.regress = nn.Linear(256, 1, F.elu)    # regression\n",
    "\n",
    "    def forward(self, seq, ecfp, graph):        \n",
    "        cv_i = self.emlayer(seq)\n",
    "        \n",
    "        cv2_i = cv_i.unsqueeze(1)\n",
    "        cv2_o = self.cv2dlayer(cv2_i)\n",
    "        cv1_i = cv2_o.squeeze()\n",
    "        cv1_o = self.cv1dlayers(cv1_i)\n",
    "        \n",
    "        ls_i = cv1_o.permute(0, 2, 1)\n",
    "        \n",
    "        ls_h = torch.zeros(2, 64, 64).cuda()     # (num_layers * num_directions, batch, hidden_size)\n",
    "        ls_c = torch.zeros(2, 64, 64).cuda()\n",
    "        \n",
    "        ls_o, (ls_h, ls_c) = self.lslayer(ls_i, (ls_h, ls_c))\n",
    "\n",
    "        for_o = ls_o[:, -1, :64]\n",
    "        back_o = ls_o[:, 0, 64:]\n",
    "        \n",
    "        concat_o = tc.cat((for_o, back_o), axis=1)   # batch, hidden*2\n",
    "        \n",
    "        ec_h = self.eclayers(ecfp)\n",
    "        dim = 1\n",
    "        for e in ec_h.size()[1:]:\n",
    "            dim = dim * e\n",
    "        ec_h = ec_h.view(-1, dim)\n",
    "        \n",
    "        gc_h = graph.in_degrees().view(-1, 1).float().cuda()     # 노드의 개수를 feature vector로 사용, ex> torch.Size([7016, 1])\n",
    "        for conv in self.gclayers:\n",
    "            gc_h = conv(graph, gc_h).cuda()\n",
    "        graph.ndata['h'] = gc_h\n",
    "        gc_h = dgl.mean_nodes(graph, 'h')\n",
    "        \n",
    "        cat = tc.cat((concat_o, ec_h, gc_h), axis=1).cuda()\n",
    "       \n",
    "        return self.regress(cat).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Set hyperparameter\n",
    "hp_d = {}\n",
    "\n",
    "# FIXME: 학습 관련 하이퍼파라미터\n",
    "hp_d['batch_size'] = 64\n",
    "hp_d['num_epochs'] = 600\n",
    "\n",
    "hp_d['init_learning_rate'] = 10 ** -3.70183\n",
    "hp_d['eps'] = 10 ** -8.39981\n",
    "hp_d['weight_decay'] = 10 ** -3.59967"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_var: 0.6580323071761779\n",
      "va_var: 0.6906900334241066\n",
      "total params: 2844003\n",
      "Training epoch 0, loss 6.0748\n",
      "Validation epoch 0, loss 0.7885\n",
      "Training epoch 1, loss 0.4914\n",
      "Validation epoch 1, loss 0.5006\n",
      "Training epoch 2, loss 0.4094\n",
      "Validation epoch 2, loss 0.4159\n",
      "Training epoch 3, loss 0.3812\n",
      "Validation epoch 3, loss 0.3934\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7b393a9bb1b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtr_epoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/prtlgn_kiba/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%% training and validation\n",
    "tr_data_loader = DataLoader(trainset, batch_size=hp_d['batch_size'], shuffle=False, collate_fn=collate)\n",
    "va_data_loader = DataLoader(validset, batch_size=hp_d['batch_size'], shuffle=False, collate_fn=collate)\n",
    "\n",
    "model = Regressor().to(torch.device('cuda:0'))\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "loss_func = nn.MSELoss(reduction='mean').cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=hp_d['init_learning_rate'], \n",
    "    weight_decay=hp_d['weight_decay'], eps=hp_d['eps'])\n",
    "\n",
    "print('tr_var:', np.var(np.array([s[3] for s in trainset])))\n",
    "print('va_var:', np.var(np.array([s[3] for s in validset])))\n",
    "print('total params:', total_params)\n",
    "\n",
    "tr_epoch_losses = []\n",
    "va_epoch_losses = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(hp_d['num_epochs']):                          #!! epoch-loop\n",
    "    # training session\n",
    "    model.train()\n",
    "    tr_epoch_loss = 0\n",
    "\n",
    "    for iter, (seq, ecfp, graph, label) in enumerate(tr_data_loader):  #!! batch-loop\n",
    "        prediction = model(seq, ecfp, graph).view(-1).cuda()\n",
    "        loss = loss_func(prediction, label).cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        tr_epoch_loss += loss.detach().item()\n",
    "    \n",
    "    tr_epoch_loss /= (iter + 1)\n",
    "    print('Training epoch {}, loss {:.4f}'.format(epoch, tr_epoch_loss))\n",
    "    tr_epoch_losses.append(tr_epoch_loss)\n",
    "\n",
    "# ===========================================================================\n",
    "    # validation session\n",
    "    model.eval()\n",
    "    va_epoch_loss = 0\n",
    "\n",
    "    for iter, (seq, ecfp, graph, label) in enumerate(va_data_loader):  # batch-loop\n",
    "        prediction = model(seq, ecfp, graph).view(-1).cuda()\n",
    "        loss = loss_func(prediction, label).cuda()\n",
    "        \n",
    "        va_epoch_loss += loss.detach().item()\n",
    "        \n",
    "    va_epoch_loss /= (iter + 1)\n",
    "    print('Validation epoch {}, loss {:.4f}'.format(epoch, va_epoch_loss))\n",
    "    va_epoch_losses.append(va_epoch_loss)\n",
    "    \n",
    "end = time.time()\n",
    "print('time elapsed:', end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 저장\n",
    "np.save('cnnlstm+ecfpgcn_tr_losses', tr_epoch_losses)\n",
    "np.save('cnnlstm+ecfpgcn_va_losses', va_epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
