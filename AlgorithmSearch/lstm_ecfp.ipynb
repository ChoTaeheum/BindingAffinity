{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% library import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch as tc\n",
    "import torch\n",
    "import pprint\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image as img\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "#%% mini_dataset loading\n",
    "with open(\"datasets/trainset_lstm+ecfp.txt\", \"rb\") as fp:\n",
    "    trainset = pickle.load(fp)\n",
    "\n",
    "with open(\"datasets/validset_lstm+ecfp.txt\", \"rb\") as fp:\n",
    "    validset = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Make collate func.\n",
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs [(graph, label),(graph, label)].\n",
    "    seqs, seq_lens, ecfps, labels = map(list, zip(*samples))\n",
    "    return tc.LongTensor(seqs).cuda(), tc.LongTensor(seq_lens), tc.tensor(ecfps, dtype=tc.float).cuda(), tc.tensor(labels).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% learning module 선언\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Regressor, self).__init__()    # method 상속받고 __init__()은 여기서 하겠다.\n",
    "        \n",
    "        self.emlayer = nn.Embedding(21, 10)\n",
    "        self.lslayer = nn.LSTM(10, 64, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.eclayers = nn.Sequential(\n",
    "                        nn.Linear(2048, 1024),\n",
    "                        nn.BatchNorm1d(1024),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.2),\n",
    "                        nn.Linear(1024, 512),\n",
    "                        nn.BatchNorm1d(512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.2),\n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.BatchNorm1d(256),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.2),\n",
    "                        nn.Linear(256, 128),\n",
    "                        )\n",
    "\n",
    "        self.regress = nn.Linear(256, 1, F.elu)    # regression\n",
    "\n",
    "    def forward(self, seq, seq_len, ecfp):\n",
    "        sorted_seq_len, sorted_idx = seq_len.sort(0, descending=True)\n",
    "        seq = seq[sorted_idx]\n",
    "        \n",
    "        ls_i = self.emlayer(seq)\n",
    "        ls_i = pack_padded_sequence(ls_i, sorted_seq_len.tolist(), batch_first=True)\n",
    "        ls_h = torch.zeros(2, 128, 64).cuda()     # (num_layers * num_directions, batch, hidden_size)\n",
    "        ls_c = torch.zeros(2, 128, 64).cuda()\n",
    "        \n",
    "        ls_o, (ls_h, ls_c) = self.lslayer(ls_i, (ls_h, ls_c))\n",
    "        ls_o, _ = pad_packed_sequence(ls_o, batch_first=True)\n",
    "\n",
    "        # 순서 다시 바로잡아주기        \n",
    "        _, sortedback_idx = sorted_idx.sort(0)\n",
    "        ls_o = ls_o[sortedback_idx]\n",
    "        \n",
    "        # 각 sample의 last output vector 추출\n",
    "        for_o = []\n",
    "        for idx, o in enumerate(ls_o):\n",
    "            for_o.append(o[seq_len[idx]-1, :64].view(1, 64))\n",
    "        for_o = torch.cat(for_o, 0)\n",
    "        \n",
    "        back_o = ls_o[:, 0, 64:]\n",
    "        \n",
    "        concat_o = tc.cat((for_o, back_o), axis=1)   # batch, hidden*2\n",
    "        \n",
    "        ec_h = self.eclayers(ecfp)\n",
    "        dim = 1\n",
    "        for e in ec_h.size()[1:]:\n",
    "            dim = dim * e\n",
    "        ec_h = ec_h.view(-1, dim)\n",
    "        \n",
    "        cat = tc.cat((concat_o, ec_h), axis=1).cuda()\n",
    "       \n",
    "        return self.regress(cat).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Set hyperparameter\n",
    "hp_d = {}\n",
    "\n",
    "# FIXME: 학습 관련 하이퍼파라미터\n",
    "hp_d['batch_size'] = 128\n",
    "hp_d['num_epochs'] = 300\n",
    "\n",
    "hp_d['init_learning_rate'] = 10 ** -3.70183\n",
    "hp_d['eps'] = 10 ** -8.39981\n",
    "hp_d['weight_decay'] = 10 ** -3.59967"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "tr_data_loader = DataLoader(trainset, batch_size=hp_d['batch_size'], shuffle=False, collate_fn=collate)\n",
    "va_data_loader = DataLoader(validset, batch_size=hp_d['batch_size'], shuffle=False, collate_fn=collate)\n",
    "\n",
    "model = Regressor().to(torch.device('cuda:0'))\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "loss_func = nn.MSELoss(reduction='mean').cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=hp_d['init_learning_rate'], \n",
    "    weight_decay=hp_d['weight_decay'], eps=hp_d['eps'])\n",
    "\n",
    "print('tr_var:', np.var(np.array([s[3] for s in trainset])))\n",
    "print('va_var:', np.var(np.array([s[3] for s in validset])))\n",
    "print('total params:', total_params)\n",
    "\n",
    "tr_epoch_losses = []\n",
    "va_epoch_losses = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(hp_d['num_epochs']):                          #!! epoch-loop\n",
    "    # training session\n",
    "    model.train()\n",
    "    tr_epoch_loss = 0\n",
    "\n",
    "    for iter, (seq, seq_len, ecfp, label) in enumerate(tr_data_loader):  #!! batch-loop\n",
    "        prediction = model(seq, seq_len, ecfp).view(-1).cuda()\n",
    "        loss = loss_func(prediction, label).cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        tr_epoch_loss += loss.detach().item()\n",
    "    \n",
    "    tr_epoch_loss /= (iter + 1)\n",
    "    print('Training epoch {}, loss {:.4f}'.format(epoch, tr_epoch_loss))\n",
    "    tr_epoch_losses.append(tr_epoch_loss)\n",
    "\n",
    "# ===========================================================================\n",
    "    # validation session\n",
    "    model.eval()\n",
    "    va_epoch_loss = 0\n",
    "\n",
    "    for iter, (seq, seq_len, ecfp, label) in enumerate(va_data_loader):  # batch-loop\n",
    "        prediction = model(seq, seq_len, ecfp).view(-1).cuda()\n",
    "        loss = loss_func(prediction, label).cuda()\n",
    "        \n",
    "        va_epoch_loss += loss.detach().item()\n",
    "        \n",
    "    va_epoch_loss /= (iter + 1)\n",
    "    print('Validation epoch {}, loss {:.4f}'.format(epoch, va_epoch_loss))\n",
    "    va_epoch_losses.append(va_epoch_loss)\n",
    "    \n",
    "end = time.time()\n",
    "print('time elapsed:', end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('lstm+ecfp_tr_losses', tr_epoch_losses)\n",
    "np.save('lstm+ecfp_va_losses', va_epoch_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
