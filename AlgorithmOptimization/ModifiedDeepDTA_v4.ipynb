{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% library import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch as tc\n",
    "import torch\n",
    "import pprint\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from sklearn.utils import shuffle\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "#%% Load dataset and cuda\n",
    "dataset = pd.read_csv(\"datasets/KIBA.csv\")\n",
    "datalen = len(dataset)\n",
    "cuda = tc.device('cuda')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% protein-ligand-kiba split\n",
    "protein = dataset.loc[:(2**16)+(2**13)-1, \"uniprotID\"]    # 5\n",
    "ligand = dataset.loc[:(2**16)+(2**13)-1, \"chemblID\"]\n",
    "kiba = list(dataset.loc[:(2**16)+(2**13)-1, 'KIBA'])\n",
    "del dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% protein sequence load\n",
    "f = open('datasets/dictionaries/prt_lstm.txt', 'rb')\n",
    "seq_voc, _ = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "sequence = np.zeros(((2**16)+(2**13), 4128))\n",
    "for i, s in enumerate(protein):\n",
    "    sequence[i] = seq_voc[s]\n",
    "\n",
    "sequence = sequence[:, :1400]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ligand ecfp load\n",
    "f = open('datasets/dictionaries/lgn_smiecoding.txt', 'rb')\n",
    "smi_dic = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "smileseq = np.zeros(((2**16)+(2**13), 590))\n",
    "for i, e in enumerate(ligand):\n",
    "    smileseq[i] = smi_dic[e]\n",
    "    \n",
    "smileseq = smileseq[:, :100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% dataset zip\n",
    "revised_dataset = list(zip(sequence, smileseq, kiba))\n",
    "shuffled_dataset = shuffle(revised_dataset); del revised_dataset\n",
    "trainset = shuffled_dataset[:2**16]\n",
    "validset = shuffled_dataset[2**16:(2**16) + (2**13)]\n",
    "\n",
    "del shuffled_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Make collate func.\n",
    "def collate(samples):\n",
    "    sequences, smileseq, labels = map(list, zip(*samples))\n",
    "    return tc.LongTensor(sequences).cuda(), tc.LongTensor(smileseq).cuda(), tc.tensor(labels).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "class Conv1d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n",
    "        super(Conv1d, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_planes, out_planes,\n",
    "                              kernel_size=9, stride=stride,\n",
    "                              padding=padding, bias=False) # verify bias false\n",
    "        self.bn = nn.BatchNorm1d(out_planes,\n",
    "                                 eps=0.001, # value found in tensorflow\n",
    "                                 momentum=0.1, # default pytorch value\n",
    "                                 affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        out = self.bn(x)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Block1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Block1, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            Conv1d(32, 48, kernel_size=10, stride=1),\n",
    "            nn.BatchNorm1d(48),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)         \n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class IBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(IBlock, self).__init__()\n",
    "        \n",
    "        self.branch0 = Conv1d(in_planes, out_planes, kernel_size=9, stride=1)\n",
    "        \n",
    "        self.branch1 = nn.Sequential(\n",
    "            Conv1d(in_planes, in_planes, kernel_size=9, stride=1),\n",
    "            nn.ReLU(),\n",
    "            Conv1d(in_planes, in_planes, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            Conv1d(in_planes, out_planes, kernel_size=9, stride=1, padding=4)\n",
    "            )\n",
    "            \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = x0 + x1\n",
    "        out = self.relu(x2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% learning module 선언\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Regressor, self).__init__()    # method 상속받고 __init__()은 여기서 하겠다.\n",
    "        \n",
    "        self.prt_emlayer = nn.Embedding(21, 10)\n",
    "        \n",
    "        self.prt_cv2dlayer = nn.Sequential(\n",
    "                        nn.Conv2d(1, 32, kernel_size = (4, 10)),\n",
    "                        nn.BatchNorm2d(num_features = 32),\n",
    "                        nn.ReLU()    # batch, channel, input_len, embedding\n",
    "                        )    \n",
    "        \n",
    "        self.prt_cv1dlayers = nn.Sequential(\n",
    "                        Block1(),\n",
    "                        IBlock(48, 192),\n",
    "                        nn.MaxPool1d(4),\n",
    "                        IBlock(192, 384),\n",
    "                        nn.AvgPool1d(163)\n",
    "                        )\n",
    "            \n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        \n",
    "        self.lgn_emlayer = nn.Embedding(64, 10)\n",
    "        \n",
    "        self.lgn_cv2dlayer = nn.Sequential(\n",
    "                        nn.Conv2d(1, 32, kernel_size = (2, 10)),\n",
    "                        nn.BatchNorm2d(num_features = 32),\n",
    "                        nn.ReLU()   \n",
    "                        )\n",
    "        \n",
    "        self.lgn_cv1dlayers = nn.Sequential(\n",
    "                        Block1(),\n",
    "                        IBlock(48, 96),\n",
    "                        nn.MaxPool1d(2),\n",
    "                        IBlock(96, 384),\n",
    "                        nn.AvgPool1d(10)\n",
    "                        )\n",
    "            \n",
    "        \n",
    "        self.mlplayers = nn.Sequential(\n",
    "                        nn.Linear(768, 1024),\n",
    "                        nn.BatchNorm1d(1024),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.2),\n",
    "                        nn.Linear(1024, 768),\n",
    "                        nn.BatchNorm1d(768),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.2),\n",
    "                        nn.Linear(768, 512),\n",
    "                        nn.BatchNorm1d(512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.2),\n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.BatchNorm1d(256),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.2),\n",
    "                        nn.Linear(256, 128),\n",
    "                        nn.BatchNorm1d(128),\n",
    "                        nn.ReLU()\n",
    "                        )\n",
    "\n",
    "        self.regress = nn.Linear(128, 1)    # regression\n",
    "\n",
    "    def forward(self, prt_seq, lgn_seq):   \n",
    "        p = self.prt_emlayer(prt_seq)\n",
    "        p = p.unsqueeze(1)\n",
    "        p = self.prt_cv2dlayer(p)\n",
    "        p = p.squeeze()\n",
    "        p = self.prt_cv1dlayers(p)     # batch, channel(->input_size), seq_len\n",
    "        p = p.squeeze()\n",
    "        \n",
    "        l = self.lgn_emlayer(lgn_seq)\n",
    "        l = l.unsqueeze(1)\n",
    "        l = self.lgn_cv2dlayer(l)\n",
    "        l = l.squeeze()\n",
    "        l = self.lgn_cv1dlayers(l)\n",
    "        l = l.squeeze()\n",
    "        \n",
    "#         print(p.size(), l.size())\n",
    "        \n",
    "        cat = tc.cat((p, l), axis=1).cuda()\n",
    "        out = self.mlplayers(cat)\n",
    "        \n",
    "        return self.regress(out).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Set hyperparameter\n",
    "hp_d = {}\n",
    "\n",
    "# FIXME: 학습 관련 하이퍼파라미터\n",
    "hp_d['batch_size'] = 128\n",
    "hp_d['num_epochs'] = 300\n",
    "\n",
    "hp_d['init_learning_rate'] = 10 ** -3.70183\n",
    "hp_d['eps'] = 10 ** -8.39981\n",
    "hp_d['weight_decay'] = 10 ** -3.59967"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_var: 0.6628701817813776\n",
      "va_var: 0.65197871005884\n",
      "total params: 6987155\n",
      "Training epoch 0, loss 76.3201\n",
      "Validation epoch 0, loss 34.5802\n",
      "Training epoch 1, loss 12.0453\n",
      "Validation epoch 1, loss 1.9841\n",
      "Training epoch 2, loss 0.9691\n",
      "Validation epoch 2, loss 0.5656\n",
      "Training epoch 3, loss 0.5381\n",
      "Validation epoch 3, loss 0.4741\n",
      "Training epoch 4, loss 0.4520\n",
      "Validation epoch 4, loss 0.5371\n",
      "Training epoch 5, loss 0.4071\n",
      "Validation epoch 5, loss 0.3897\n",
      "Training epoch 6, loss 0.3775\n",
      "Validation epoch 6, loss 0.3551\n",
      "Training epoch 7, loss 0.3878\n",
      "Validation epoch 7, loss 0.3899\n",
      "Training epoch 8, loss 0.3687\n",
      "Validation epoch 8, loss 0.3397\n",
      "Training epoch 9, loss 0.3671\n",
      "Validation epoch 9, loss 0.3698\n",
      "Training epoch 10, loss 0.3480\n",
      "Validation epoch 10, loss 0.3276\n",
      "Training epoch 11, loss 0.3285\n",
      "Validation epoch 11, loss 0.3135\n",
      "Training epoch 12, loss 0.3284\n",
      "Validation epoch 12, loss 0.9324\n",
      "Training epoch 13, loss 0.3295\n",
      "Validation epoch 13, loss 0.3720\n",
      "Training epoch 14, loss 0.3315\n",
      "Validation epoch 14, loss 0.3364\n",
      "Training epoch 15, loss 0.3286\n",
      "Validation epoch 15, loss 0.3291\n",
      "Training epoch 16, loss 0.3203\n",
      "Validation epoch 16, loss 0.3478\n",
      "Training epoch 17, loss 0.3266\n",
      "Validation epoch 17, loss 0.3712\n",
      "Training epoch 18, loss 0.3044\n",
      "Validation epoch 18, loss 0.3187\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-5bc7ee83c8f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtr_epoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#!! batch-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/prtlgn_kiba/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/prtlgn_kiba/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/prtlgn_kiba/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-942719fba060>\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(samples)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmileseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmileseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%% training and validation\n",
    "tr_data_loader = DataLoader(trainset, batch_size=hp_d['batch_size'], shuffle=False, collate_fn=collate)\n",
    "va_data_loader = DataLoader(validset, batch_size=hp_d['batch_size'], shuffle=False, collate_fn=collate)\n",
    "\n",
    "model = Regressor().to(torch.device('cuda:0'))\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "loss_func = nn.MSELoss(reduction='mean').cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=hp_d['init_learning_rate'], \n",
    "    weight_decay=hp_d['weight_decay'], eps=hp_d['eps'])\n",
    "\n",
    "print('tr_var:', np.var(np.array([s[2] for s in trainset])))\n",
    "print('va_var:', np.var(np.array([s[2] for s in validset])))\n",
    "print('total params:', total_params)\n",
    "\n",
    "tr_epoch_losses = []\n",
    "va_epoch_losses = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(hp_d['num_epochs']):                          #!! epoch-loop\n",
    "    # training session\n",
    "    model.train()\n",
    "    tr_epoch_loss = 0\n",
    "\n",
    "    for iter, (seq, smi, label) in enumerate(tr_data_loader):  #!! batch-loop\n",
    "        prediction = model(seq, smi).view(-1).cuda()\n",
    "        loss = loss_func(prediction, label).cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        tr_epoch_loss += loss.detach().item()\n",
    "    \n",
    "    tr_epoch_loss /= (iter + 1)\n",
    "    print('Training epoch {}, loss {:.4f}'.format(epoch, tr_epoch_loss))\n",
    "    tr_epoch_losses.append(tr_epoch_loss)\n",
    "\n",
    "# ===========================================================================\n",
    "    # validation session\n",
    "    model.eval()\n",
    "    va_epoch_loss = 0\n",
    "\n",
    "    for iter, (seq, smi, label) in enumerate(va_data_loader):  # batch-loop\n",
    "        prediction = model(seq, smi).view(-1).cuda()\n",
    "        loss = loss_func(prediction, label).cuda()\n",
    "        \n",
    "        va_epoch_loss += loss.detach().item()\n",
    "        \n",
    "    va_epoch_loss /= (iter + 1)\n",
    "    print('Validation epoch {}, loss {:.4f}'.format(epoch, va_epoch_loss))\n",
    "    va_epoch_losses.append(va_epoch_loss)\n",
    "    \n",
    "end = time.time()\n",
    "print('time elapsed:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "np.save('ModifiedDeepDTA_v4_tr_losses', tr_epoch_losses)\n",
    "np.save('ModifiedDeepDTA_v4_va_losses', va_epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19527420005761087"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(va_epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
