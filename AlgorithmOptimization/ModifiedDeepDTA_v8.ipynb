{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% library import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch as tc\n",
    "import torch\n",
    "import pprint\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from sklearn.utils import shuffle\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "from bayes_opt import BayesianOptimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "#%% Load dataset and cuda\n",
    "dataset = pd.read_csv(\"datasets/KIBA.csv\")\n",
    "datalen = len(dataset)\n",
    "cuda = tc.device('cuda')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% protein-ligand-kiba split\n",
    "protein = dataset.loc[:, \"uniprotID\"]    # 5\n",
    "ligand = dataset.loc[:, \"chemblID\"]\n",
    "kiba = list(dataset.loc[:, 'KIBA'])\n",
    "del dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% protein sequence load\n",
    "f = open('datasets/dictionaries/prt_lstm.txt', 'rb')\n",
    "seq_voc, _ = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "sequence = np.zeros((datalen, 4128))\n",
    "for i, s in enumerate(protein):\n",
    "    sequence[i] = seq_voc[s]\n",
    "\n",
    "sequence = sequence[:, :1400]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ligand smiles load\n",
    "f = open('datasets/dictionaries/lgn_smiecoding.txt', 'rb')\n",
    "smi_dic = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "smileseq = np.zeros((datalen, 590))\n",
    "for i, e in enumerate(ligand):\n",
    "    smileseq[i] = smi_dic[e]\n",
    "\n",
    "smileseq = smileseq[:, :100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% dataset zip\n",
    "revised_dataset = list(zip(sequence, smileseq, kiba))\n",
    "shuffled_dataset = shuffle(revised_dataset); del revised_dataset\n",
    "trainset = shuffled_dataset[:int((9/10)*datalen)]\n",
    "validset = shuffled_dataset[int((9/10)*datalen):]\n",
    "\n",
    "del shuffled_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Make collate func.\n",
    "def collate(samples):\n",
    "    sequences, smileseq, labels = map(list, zip(*samples))\n",
    "    return tc.LongTensor(sequences).cuda(), tc.LongTensor(smileseq).cuda(), tc.tensor(labels).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% learning module 선언\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Regressor, self).__init__()    # method 상속받고 __init__()은 여기서 하겠다.\n",
    "        \n",
    "        self.prt_emlayer = nn.Embedding(21, 10)\n",
    "        \n",
    "        self.prt_cv1dlayers = nn.Sequential(\n",
    "                        nn.Conv1d(10, 32, kernel_size = 4),\n",
    "                        nn.BatchNorm1d(num_features = 32),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv1d(32, 64, kernel_size = 8),\n",
    "                        nn.BatchNorm1d(num_features = 64),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv1d(64, 96, kernel_size = 12),\n",
    "                        nn.BatchNorm1d(num_features = 96),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool1d(kernel_size=1379)\n",
    "                        )\n",
    "        \n",
    "        \n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        \n",
    "        self.lgn_emlayer = nn.Embedding(64, 10)\n",
    "        \n",
    "        self.lgn_cv1dlayers = nn.Sequential(\n",
    "                        nn.Conv1d(10, 32, kernel_size = 4),\n",
    "                        nn.BatchNorm1d(num_features = 32),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv1d(32, 64, kernel_size = 6),\n",
    "                        nn.BatchNorm1d(num_features = 64),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv1d(64, 96, kernel_size = 8),\n",
    "                        nn.BatchNorm1d(num_features = 96),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool1d(kernel_size = 85)\n",
    "                        )\n",
    "        \n",
    "        \n",
    "        self.mlplayers = nn.Sequential(\n",
    "                        nn.Linear(192, 1024),\n",
    "                        nn.BatchNorm1d(1024),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.1),\n",
    "                        nn.Linear(1024, 1024),\n",
    "                        nn.BatchNorm1d(1024),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.1),\n",
    "                        nn.Linear(1024, 1024),\n",
    "                        nn.BatchNorm1d(1024),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.1),\n",
    "                        nn.Linear(1024, 512),\n",
    "                        nn.BatchNorm1d(512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.1),\n",
    "                        nn.Linear(512, 512),\n",
    "                        nn.BatchNorm1d(512),\n",
    "                        nn.ReLU()\n",
    "                        )\n",
    "\n",
    "        self.regress = nn.Linear(512, 1)    # regression\n",
    "\n",
    "    def forward(self, prt_seq, lgn_seq):   \n",
    "        p = self.prt_emlayer(prt_seq)\n",
    "        p = p.permute(0, 2, 1)\n",
    "        p = self.prt_cv1dlayers(p)\n",
    "        p = p.squeeze()\n",
    "        \n",
    "        l = self.lgn_emlayer(lgn_seq)\n",
    "        l = l.permute(0, 2, 1)\n",
    "        l = self.lgn_cv1dlayers(l)\n",
    "        l = l.squeeze()\n",
    "        \n",
    "        cat = tc.cat((p, l), axis=1).cuda()\n",
    "        out = self.mlplayers(cat)\n",
    "        \n",
    "        return self.regress(out).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Set hyperparameter\n",
    "hp_d = {}\n",
    "\n",
    "# FIXME: 학습 관련 하이퍼파라미터\n",
    "hp_d['batch_size'] = 512\n",
    "hp_d['num_epochs'] = 100\n",
    "\n",
    "hp_d['init_learning_rate'] = 0\n",
    "hp_d['eps'] = 0\n",
    "hp_d['weight_decay'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_var: 0.7030632370108503\n",
      "va_var: 0.6771456665055565\n"
     ]
    }
   ],
   "source": [
    "#%% training and validation\n",
    "tr_data_loader = DataLoader(trainset, batch_size=hp_d['batch_size'], shuffle=False, collate_fn=collate)\n",
    "va_data_loader = DataLoader(validset, batch_size=hp_d['batch_size'], shuffle=False, collate_fn=collate)\n",
    "\n",
    "print('tr_var:', np.var(np.array([s[2] for s in trainset])))\n",
    "print('va_var:', np.var(np.array([s[2] for s in validset])))\n",
    "\n",
    "def train_and_validate(init_learning_rate_log, weight_decay_log, eps):\n",
    "    \n",
    "    model = Regressor().to(torch.device('cuda:0'))\n",
    "    loss_func = nn.MSELoss(reduction='mean').cuda()\n",
    "    \n",
    "    hp_d['init_learning_rate'] = 10**init_learning_rate_log\n",
    "    hp_d['weight_decay'] = 10**weight_decay_log\n",
    "    hp_d['eps'] = 10**eps\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=hp_d['init_learning_rate'], \n",
    "        weight_decay=hp_d['weight_decay'], eps=hp_d['eps'])\n",
    "\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    tr_epoch_losses = []\n",
    "    va_epoch_losses = []\n",
    "\n",
    "    for epoch in range(hp_d['num_epochs']):                             #!! epoch-loop\n",
    "        # training session\n",
    "        model.train()\n",
    "        tr_epoch_loss = 0\n",
    "\n",
    "        for iter, (seq, smi, label) in enumerate(tr_data_loader):       #!! batch-loop\n",
    "            prediction = model(seq, smi).view(-1).cuda()\n",
    "            loss = loss_func(prediction, label).cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tr_epoch_loss += loss.detach().item()\n",
    "\n",
    "        tr_epoch_loss /= (iter + 1)\n",
    "#         print('Training epoch {}, loss {:.4f}'.format(epoch, tr_epoch_loss))\n",
    "        tr_epoch_losses.append(tr_epoch_loss)\n",
    "\n",
    "    # ===========================================================================\n",
    "        # validation session\n",
    "        model.eval()\n",
    "        va_epoch_loss = 0\n",
    "\n",
    "        for iter, (seq, smi, label) in enumerate(va_data_loader):  # batch-loop\n",
    "            prediction = model(seq, smi).view(-1).cuda()\n",
    "            loss = loss_func(prediction, label).cuda()\n",
    "\n",
    "            va_epoch_loss += loss.detach().item()\n",
    "\n",
    "        va_epoch_loss /= (iter + 1)\n",
    "#         print('Validation epoch {}, loss {:.4f}'.format(epoch, va_epoch_loss))\n",
    "        va_epoch_losses.append(va_epoch_loss)\n",
    "    \n",
    "    return -min(va_epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    eps    | init_l... | weight... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.2047  \u001b[0m | \u001b[0m-7.902   \u001b[0m | \u001b[0m-2.139   \u001b[0m | \u001b[0m-4.692   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.1615  \u001b[0m | \u001b[95m-7.91    \u001b[0m | \u001b[95m-3.305   \u001b[0m | \u001b[95m-4.562   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.377   \u001b[0m | \u001b[0m-8.125   \u001b[0m | \u001b[0m-1.433   \u001b[0m | \u001b[0m-3.609   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.2109  \u001b[0m | \u001b[0m-7.0     \u001b[0m | \u001b[0m-5.0     \u001b[0m | \u001b[0m-6.5     \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m-0.1566  \u001b[0m | \u001b[95m-9.0     \u001b[0m | \u001b[95m-3.193   \u001b[0m | \u001b[95m-6.5     \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.1664  \u001b[0m | \u001b[0m-7.0     \u001b[0m | \u001b[0m-2.122   \u001b[0m | \u001b[0m-6.5     \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.2148  \u001b[0m | \u001b[0m-9.0     \u001b[0m | \u001b[0m-5.0     \u001b[0m | \u001b[0m-4.724   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.163   \u001b[0m | \u001b[0m-7.569   \u001b[0m | \u001b[0m-3.314   \u001b[0m | \u001b[0m-5.691   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.2177  \u001b[0m | \u001b[0m-7.0     \u001b[0m | \u001b[0m-5.0     \u001b[0m | \u001b[0m-3.5     \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.2393  \u001b[0m | \u001b[0m-9.0     \u001b[0m | \u001b[0m-1.0     \u001b[0m | \u001b[0m-6.5     \u001b[0m |\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m-0.1529  \u001b[0m | \u001b[95m-9.0     \u001b[0m | \u001b[95m-3.09    \u001b[0m | \u001b[95m-5.257   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.1625  \u001b[0m | \u001b[0m-7.002   \u001b[0m | \u001b[0m-3.369   \u001b[0m | \u001b[0m-4.674   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.1714  \u001b[0m | \u001b[0m-9.0     \u001b[0m | \u001b[0m-4.001   \u001b[0m | \u001b[0m-3.5     \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.158   \u001b[0m | \u001b[0m-8.885   \u001b[0m | \u001b[0m-2.488   \u001b[0m | \u001b[0m-6.486   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.1579  \u001b[0m | \u001b[0m-7.024   \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m-6.436   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.1558  \u001b[0m | \u001b[0m-8.995   \u001b[0m | \u001b[0m-3.675   \u001b[0m | \u001b[0m-4.579   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.1572  \u001b[0m | \u001b[0m-8.164   \u001b[0m | \u001b[0m-2.923   \u001b[0m | \u001b[0m-6.489   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.1713  \u001b[0m | \u001b[0m-7.005   \u001b[0m | \u001b[0m-3.373   \u001b[0m | \u001b[0m-3.546   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-0.2163  \u001b[0m | \u001b[0m-8.927   \u001b[0m | \u001b[0m-4.989   \u001b[0m | \u001b[0m-6.441   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.1602  \u001b[0m | \u001b[0m-7.015   \u001b[0m | \u001b[0m-2.628   \u001b[0m | \u001b[0m-5.538   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-0.1531  \u001b[0m | \u001b[0m-8.99    \u001b[0m | \u001b[0m-3.556   \u001b[0m | \u001b[0m-5.463   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-0.1601  \u001b[0m | \u001b[0m-7.993   \u001b[0m | \u001b[0m-4.006   \u001b[0m | \u001b[0m-3.542   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-0.1596  \u001b[0m | \u001b[0m-8.673   \u001b[0m | \u001b[0m-3.004   \u001b[0m | \u001b[0m-5.86    \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.1595  \u001b[0m | \u001b[0m-7.659   \u001b[0m | \u001b[0m-2.449   \u001b[0m | \u001b[0m-6.464   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.1655  \u001b[0m | \u001b[0m-8.34    \u001b[0m | \u001b[0m-3.375   \u001b[0m | \u001b[0m-3.533   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.2347  \u001b[0m | \u001b[0m-7.012   \u001b[0m | \u001b[0m-1.057   \u001b[0m | \u001b[0m-6.335   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.1573  \u001b[0m | \u001b[0m-8.005   \u001b[0m | \u001b[0m-3.944   \u001b[0m | \u001b[0m-6.478   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-0.1696  \u001b[0m | \u001b[0m-7.948   \u001b[0m | \u001b[0m-4.268   \u001b[0m | \u001b[0m-5.036   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.1586  \u001b[0m | \u001b[0m-7.198   \u001b[0m | \u001b[0m-3.987   \u001b[0m | \u001b[0m-3.935   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-0.1605  \u001b[0m | \u001b[0m-7.015   \u001b[0m | \u001b[0m-4.038   \u001b[0m | \u001b[0m-5.728   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-0.1638  \u001b[0m | \u001b[0m-8.952   \u001b[0m | \u001b[0m-2.967   \u001b[0m | \u001b[0m-4.501   \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-0.173   \u001b[0m | \u001b[0m-8.977   \u001b[0m | \u001b[0m-2.168   \u001b[0m | \u001b[0m-5.808   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-0.1577  \u001b[0m | \u001b[0m-7.116   \u001b[0m | \u001b[0m-3.999   \u001b[0m | \u001b[0m-6.497   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-0.1595  \u001b[0m | \u001b[0m-8.435   \u001b[0m | \u001b[0m-3.789   \u001b[0m | \u001b[0m-4.037   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-0.1556  \u001b[0m | \u001b[0m-8.996   \u001b[0m | \u001b[0m-3.943   \u001b[0m | \u001b[0m-6.277   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-0.1725  \u001b[0m | \u001b[0m-7.002   \u001b[0m | \u001b[0m-4.316   \u001b[0m | \u001b[0m-4.687   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-0.167   \u001b[0m | \u001b[0m-8.167   \u001b[0m | \u001b[0m-2.036   \u001b[0m | \u001b[0m-6.488   \u001b[0m |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/prtlgn_kiba/lib/python3.6/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (-7.593811887524278, -3.510543621709643, -3.9283232334924314)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0ffded797867>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mbayes_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ei'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# FIXME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbayes_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/prtlgn_kiba/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZATION_END\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/prtlgn_kiba/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/prtlgn_kiba/lib/python3.6/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-653d792d3418>\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[0;34m(init_learning_rate_log, weight_decay_log, eps)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtr_epoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0;31m#!! batch-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/prtlgn_kiba/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/prtlgn_kiba/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/prtlgn_kiba/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-942719fba060>\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(samples)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmileseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmileseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bayes_optimizer = BayesianOptimization(\n",
    "    f=train_and_validate,\n",
    "    pbounds={\n",
    "        'init_learning_rate_log': (-5, -1),      # FIXME\n",
    "        'weight_decay_log': (-6.5, -3.5),            # FIXME\n",
    "        'eps': (-9, -7)                          # FIXME\n",
    "    },\n",
    "    random_state=0,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "bayes_optimizer.maximize(init_points=3, n_iter=60, acq='ei', xi=0.01)    # FIXME\n",
    "\n",
    "for i, res in enumerate(bayes_optimizer.res):\n",
    "    print('Iteration {}: \\n\\t{}'.format(i, res))\n",
    "print('Final result: ', bayes_optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "np.save('ModifiedDeepDTA_v8_1_va_losses(optim)', bayes_optimizer.res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
